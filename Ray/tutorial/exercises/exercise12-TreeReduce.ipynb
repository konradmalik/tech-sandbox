{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 12 - Tree Reduce\n",
    "\n",
    "**GOAL:** The goal of this exercise is to show how to implement a tree reduce in Ray by passing object IDs into remote functions to encode dependencies between tasks.\n",
    "\n",
    "In this exercise, you will use Ray to implement parallel data generation and a parallel tree reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import ray\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-28 09:18:57,591\tWARNING services.py:586 -- setpgrp failed, processes may not be cleaned up properly: [Errno 1] Operation not permitted.\n",
      "2020-02-28 09:18:57,662\tINFO resource_spec.py:212 -- Starting Ray with 9.13 GiB memory available for workers and up to 4.58 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-02-28 09:18:58,037\tWARNING services.py:1403 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.17.0.2',\n",
       " 'redis_address': '172.17.0.2:23154',\n",
       " 'object_store_address': '/tmp/ray/session_2020-02-28_09-18-57_589947_1126/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-02-28_09-18-57_589947_1126/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2020-02-28_09-18-57_589947_1126'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(num_cpus=8, include_webui=False, ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** These functions will need to be turned into remote functions so that the tree of tasks can be executed in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a proxy for a function which generates some data.\n",
    "@ray.remote\n",
    "def create_data(i):\n",
    "    time.sleep(0.3)\n",
    "    return i * np.ones(10000)\n",
    "\n",
    "# This is a proxy for an expensive aggregation step (which is also\n",
    "# commutative and associative so it can be used in a tree-reduce).\n",
    "@ray.remote\n",
    "def aggregate_data(x, y):\n",
    "    time.sleep(0.3)\n",
    "    return x * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Make the data creation tasks run in parallel. Also aggregate the vectors in parallel. Note that the `aggregate_data` function must be called 7 times. They cannot all run in parallel because some depend on the outputs of others. However, it is possible to first run 4 in parallel, then 2 in parallel, and then 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sleep a little to improve the accuracy of the timing measurements below.\n",
    "time.sleep(1.0)\n",
    "start_time = time.time()\n",
    "\n",
    "# EXERCISE: Here we generate some data. Do this part in parallel.\n",
    "vectors = [create_data.remote(i + 1) for i in range(8)]\n",
    "\n",
    "# Here we aggregate all of the data repeatedly calling aggregate_data. This\n",
    "# can be sped up using Ray.\n",
    "#\n",
    "# NOTE: A direct translation of the code below to use Ray will not result in\n",
    "# a speedup because each function call uses the output of the previous function\n",
    "# call so the function calls must be executed serially.\n",
    "#\n",
    "# EXERCISE: Speed up the aggregation below by using Ray. Note that this will\n",
    "# require restructuring the code to expose more parallelism. First run 4 tasks\n",
    "# aggregating the 8 values in pairs. Then run 2 tasks aggregating the resulting\n",
    "# 4 intermediate values in pairs. then run 1 task aggregating the two resulting\n",
    "# values. Lastly, you will need to call ray.get to retrieve the final result.\n",
    "#\n",
    "# Exposing more parallelism means aggregating the vectors in a DIFFERENT ORDER.\n",
    "# This can be done because we are simply summing the data and the order in\n",
    "# which the values are summed doesn't matter (it's commutative and associative).\n",
    "\n",
    "while len(vectors) > 1:\n",
    "    vectors = vectors[2:] + [aggregate_data.remote(vectors[0], vectors[1])]\n",
    "result = ray.get(vectors[0])\n",
    "\n",
    "\n",
    "# NOTE: For clarity, the aggregation above is written out as 7 separate function\n",
    "# calls, but this can be done more easily in a while loop via\n",
    "#\n",
    "#     while len(vectors) > 1:\n",
    "#         vectors = aggregate_data(vectors[0], vectors[1]) + vectors[2:]\n",
    "#     result = vectors[0]\n",
    "#\n",
    "# When expressed this way, the change from serial aggregation to tree-structured\n",
    "# aggregation can be made simply by appending the result of aggregate_data to the\n",
    "# end of the vectors list as opposed to the beginning.\n",
    "#\n",
    "# EXERCISE: Think about why this is true.\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VERIFY:** Run some checks to verify that the changes you made to the code were correct. Some of the checks should fail when you initially run the cells. After completing the exercises, the checks should pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! The example took 1.2362489700317383 seconds.\n"
     ]
    }
   ],
   "source": [
    "assert np.all(result == 40320 * np.ones(10000)), ('Did you remember to '\n",
    "                                                  'call ray.get?')\n",
    "assert duration < 0.3 + 0.9 + 0.3, ('FAILURE: The data generation and '\n",
    "                                    'aggregation took {} seconds. This is '\n",
    "                                    'too slow'.format(duration))\n",
    "assert duration > 0.3 + 0.9, ('FAILURE: The data generation and '\n",
    "                              'aggregation took {} seconds. This is '\n",
    "                              'too fast'.format(duration))\n",
    "\n",
    "print('Success! The example took {} seconds.'.format(duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Use the UI to view the task timeline and to verify that the vectors were aggregated with a tree of tasks.\n",
    "\n",
    "You should be able to see the 8 `create_data` tasks running in parallel followed by 4 `aggregate_data` tasks running in parallel followed by 2 more `aggregate_data` tasks followed by 1 more `aggregate_data` task.\n",
    "\n",
    "In the timeline, click on **View Options** and select **Flow Events** to visualize tasks dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ray.experimental.ui'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f78e3281fe4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mui\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mui\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mui\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_timeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ray.experimental.ui'"
     ]
    }
   ],
   "source": [
    "import ray.experimental.ui as ui\n",
    "ui.task_timeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
